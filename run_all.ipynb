{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5265634a-5c85-4966-b531-29896a045d16",
   "metadata": {},
   "source": [
    "# Run baseline 1: generate LLM and interface\n",
    "\n",
    "路径格式:\n",
    "- 当前目录:  /mnt/workspace\n",
    "- 环境目录: /opt/conda/envs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511e20f-377d-4329-a93e-15298dcdfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /mnt/workspace/install.sh\n",
    "# 切换到 conda 的环境文件夹\n",
    "cd  /opt/conda/envs \n",
    "mkdir ipex\n",
    "# 下载 ipex-llm 官方环境\n",
    "# wget https://s3.idzcn.com/ipex-llm/ipex-llm-2.1.0b20240410.tar.gz \n",
    "sudo apt-get update\n",
    "sudo apt-get install aria2\n",
    "aria2c -x 16 https://s3.idzcn.com/ipex-llm/ipex-llm-2.1.0b20240410.tar.gz\n",
    "\n",
    "# 解压文件夹以便恢复原先环境\n",
    "tar -zxvf ipex-llm-2.1.0b20240410.tar.gz -C ipex/ && rm ipex-llm-2.1.0b20240410.tar.gz\n",
    "# 安装 ipykernel 并将其注册到 notebook 可使用内核中\n",
    "/opt/conda/envs/ipex/bin/python3 -m pip install ipykernel && /opt/conda/envs/ipex/bin/python3 -m ipykernel install --name=ipex\n",
    "\n",
    "/opt/conda/envs/ipex/bin/python3 -m pip install gradio streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ea175-1947-4860-b9a8-b0ae8bfd05b8",
   "metadata": {},
   "source": [
    "```这里手动选中jupyter notebook kernel 到ipex环境， 或者在terminal里面 conda activate ipex 进入环境```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa851c8c-8836-47f8-ae8c-9fe6babcfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /mnt/workspace\n",
    "!conda activate ipex\n",
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a1b36-1f06-4cb9-8236-c82d4466b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "# 第一个参数表示下载模型的型号，第二个参数是下载后存放的缓存地址，第三个表示版本号，默认 master\n",
    "model_dir = snapshot_download('Qwen/Qwen2-1.5B-Instruct', cache_dir='qwen2chat_src', revision='master')\n",
    "\n",
    "\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from transformers import  AutoTokenizer\n",
    "import os\n",
    "# quantitize model to reduce model size and inference time\n",
    "model_path = os.path.join(os.getcwd(),\"qwen2chat_src/Qwen/Qwen2-1___5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model.save_low_bit('qwen2chat_int4')\n",
    "tokenizer.save_pretrained('qwen2chat_int4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7262b5d-6833-41ec-b634-ff1c381c7984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50902a5-9a4b-430a-a3e3-87a9f67c3d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c55b3-d157-4e64-a47e-27e49750920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /mnt/workspace/run_gradio_stream.py\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from threading import Thread, Event\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # 设置OpenMP线程数为8,用于控制并行计算\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "load_path = \"qwen2chat_int4\"  # 模型路径\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)  # 加载低位模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)  # 加载对应的tokenizer\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 检查是否有GPU可用\n",
    "model = model.to(device)  # 将模型移动到选定的设备上\n",
    "\n",
    "# 创建 TextIteratorStreamer，用于流式生成文本\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 创建一个停止事件，用于控制生成过程的中断\n",
    "stop_event = Event()\n",
    "\n",
    "# 定义用户输入处理函数\n",
    "def user(user_message, history):\n",
    "    return \"\", history + [[user_message, None]]  # 返回空字符串和更新后的历史记录\n",
    "\n",
    "# 定义机器人回复生成函数\n",
    "def bot(history):\n",
    "    stop_event.clear()  # 重置停止事件\n",
    "    prompt = history[-1][0]  # 获取最新的用户输入\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]  # 构建消息格式\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  # 应用聊天模板\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)  # 对输入进行编码并移到指定设备\n",
    "    \n",
    "    print(f\"\\n用户输入: {prompt}\")\n",
    "    print(\"模型输出: \", end=\"\", flush=True)\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "\n",
    "    # 设置生成参数\n",
    "    generation_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=512,  # 最大生成512个新token\n",
    "        do_sample=True,  # 使用采样\n",
    "        top_p=0.7,  # 使用top-p采样\n",
    "        temperature=0.95,  # 控制生成的随机性\n",
    "    )\n",
    "\n",
    "    # 在新线程中运行模型生成\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:  # 迭代生成的文本流\n",
    "        if stop_event.is_set():  # 检查是否需要停止生成\n",
    "            print(\"\\n生成被用户停止\")\n",
    "            break\n",
    "        generated_text += new_text\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "        history[-1][1] = generated_text  # 更新历史记录中的回复\n",
    "        yield history  # 逐步返回更新的历史记录\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n\\n生成完成，用时: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "# 定义停止生成函数\n",
    "def stop_generation():\n",
    "    stop_event.set()  # 设置停止事件\n",
    "\n",
    "# 使用Gradio创建Web界面\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Qwen 聊天机器人\")\n",
    "    chatbot = gr.Chatbot()  # 聊天界面组件\n",
    "    msg = gr.Textbox()  # 用户输入文本框\n",
    "    clear = gr.Button(\"清除\")  # 清除按钮\n",
    "    stop = gr.Button(\"停止生成\")  # 停止生成按钮\n",
    "\n",
    "    # 设置用户输入提交后的处理流程\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)  # 清除按钮功能\n",
    "    stop.click(stop_generation, queue=False)  # 停止生成按钮功能\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"启动 Gradio 界面...\")\n",
    "    demo.queue()  # 启用队列处理请求\n",
    "    demo.launch(root_path='/dsw-525085/proxy/7860/')  # 兼容魔搭情况下的路由"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da79fc-8ac1-4964-8379-f59cf765d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/ipex/bin/python3 run_gradio_stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4f004-9cce-41eb-93f9-ffbfe025f8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d427f3f-6273-48eb-aa6d-1bbaef5153cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /mnt/workspace/run_stream.py\n",
    "# 设置OpenMP线程数为8\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# 导入Intel扩展的Transformers模型\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 加载模型路径\n",
    "load_path = \"qwen2chat_int4\"\n",
    "\n",
    "# 加载4位量化的模型\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "\n",
    "# 加载对应的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# 创建文本流式输出器\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 设置提示词\n",
    "prompt = \"给我讲一个芯片制造的流程\"\n",
    "\n",
    "# 构建消息列表\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "# 使用推理模式\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # 应用聊天模板,添加生成提示\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 对输入文本进行编码\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    print(\"start generate\")\n",
    "    st = time.time()  # 记录开始时间\n",
    "    \n",
    "    # 生成文本\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,  # 最大生成512个新token\n",
    "        streamer=streamer,   # 使用流式输出\n",
    "    )\n",
    "    \n",
    "    end = time.time()  # 记录结束时间\n",
    "    \n",
    "    # 打印推理时间\n",
    "    print(f'Inference time: {end-st} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd073b-c29b-4c76-8d05-086860bd0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/ipex/bin/python3 run_stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26cbb4-35ce-4e6e-bde1-44b459778ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9450292-0fdf-4286-a0d5-2d2ad898a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /mnt/workspace/run_streamlit_stream.py\n",
    "\n",
    "\n",
    "# 导入操作系统模块，用于设置环境变量\n",
    "import os\n",
    "# 设置环境变量 OMP_NUM_THREADS 为 8，用于控制 OpenMP 线程数\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# 导入时间模块\n",
    "import time\n",
    "# 导入 Streamlit 模块，用于创建 Web 应用\n",
    "import streamlit as st\n",
    "# 从 transformers 库中导入 AutoTokenizer 类\n",
    "from transformers import AutoTokenizer\n",
    "# 从 transformers 库中导入 TextStreamer 类\n",
    "from transformers import TextStreamer, TextIteratorStreamer\n",
    "# 从 ipex_llm.transformers 库中导入 AutoModelForCausalLM 类\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "# 导入 PyTorch 库\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "# 指定模型路径\n",
    "load_path = \"qwen2chat_int4\"\n",
    "# 加载低比特率模型\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "# 从预训练模型中加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# 定义生成响应函数\n",
    "def generate_response(messages, message_placeholder):\n",
    "    # 将用户的提示转换为消息格式\n",
    "    # messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # 应用聊天模板并进行 token 化\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    # 创建 TextStreamer 对象，跳过提示和特殊标记\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # 使用 zip 函数同时遍历 model_inputs.input_ids 和 generated_ids\n",
    "    generation_kwargs = dict(inputs=model_inputs.input_ids, max_new_tokens=512, streamer=streamer)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    return streamer\n",
    "\n",
    "# Streamlit 应用部分\n",
    "# 设置应用标题\n",
    "st.title(\"大模型聊天应用\")\n",
    "\n",
    "# 初始化聊天历史，如果不存在则创建一个空列表\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# 显示聊天历史\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# 用户输入部分\n",
    "if prompt := st.chat_input(\"你想说点什么?\"):\n",
    "    # 将用户消息添加到聊天历史\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    response  = str()\n",
    "    # 创建空的占位符用于显示生成的响应\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        \n",
    "        # 调用模型生成响应\n",
    "        streamer = generate_response(st.session_state.messages, message_placeholder)\n",
    "        for text in streamer:\n",
    "            response += text\n",
    "            message_placeholder.markdown(response + \"▌\")\n",
    "    \n",
    "        message_placeholder.markdown(response)\n",
    "    \n",
    "    # 将助手的响应添加到聊天历史\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd75d9-b137-4334-8407-208acfb93cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/ipex/bin/python3 run_streamlit_stream.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f58c19-1c06-4031-ae87-acf46449001c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d14c4-963d-4417-9f16-6472e4147113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f99e24-ef46-4691-b909-3a645661468e",
   "metadata": {},
   "source": [
    "# Run baseline 2: generate RAG\n",
    "安装额外 RAG 依赖包， 包括rag的数据库索引向量包， pdf解析包等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295ec9-ac0b-4791-928d-dd78e90a3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /mnt/workspace\n",
    "!conda activate ipex\n",
    "/opt/conda/envs/ipex/bin/python3 -m pip install PyMuPDF llama-index-vector-stores-chroma llama-index-readers-file llama-index-embeddings-huggingface llama-index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296fcf7-6fe6-4a5e-9861-9da853e9d7f9",
   "metadata": {},
   "source": [
    "下载用来作为数据库检索向量的文档， RAG会根据query和这些文档向量emb做相似度， 之后再二次检索加入更多信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c4c4ca-e048-4bea-a286-f2930479bf5f",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T05:58:12.614796Z",
     "iopub.status.busy": "2024-07-21T05:58:12.614446Z",
     "iopub.status.idle": "2024-07-21T05:59:53.687514Z",
     "shell.execute_reply": "2024-07-21T05:59:53.686818Z",
     "shell.execute_reply.started": "2024-07-21T05:58:12.614774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-21 13:58:12--  https://arxiv.org/pdf/2302.13971\n",
      "正在解析主机 arxiv.org (arxiv.org)... 151.101.131.42, 151.101.67.42, 151.101.3.42, ...\n",
      "正在连接 arxiv.org (arxiv.org)|151.101.131.42|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 726566 (710K) [application/pdf]\n",
      "正在保存至: ‘2302.13971.2’\n",
      "\n",
      "2302.13971.2        100%[===================>] 709.54K  7.06KB/s    用时 99s     \n",
      "\n",
      "2024-07-21 13:59:53 (7.17 KB/s) - 已保存 ‘2302.13971.2’ [726566/726566])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget https://arxiv.org/pdf/2302.13971 \n",
    "!mv 2302.13971  ./data/llamatiny_v1.pdf\n",
    "!wget https://arxiv.org/pdf/2307.09288\n",
    "!mv 2307.09288  ./data/llamatiny_v2.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05459a3f-b9a5-47e1-bbe8-d6fa23aebefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "834a0ec5-693b-4f6e-809a-57f4a1d5088b",
   "metadata": {},
   "source": [
    "下载 qwen 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e87048-76cb-4ddd-8b71-6aa0d9f24c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "# 第一个参数表示下载模型的型号，第二个参数是下载后存放的缓存地址，第三个表示版本号，默认 master\n",
    "model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b3a1c-cf80-417e-93b1-f3820517c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /mnt/workspace/run_rag.py\n",
    "# 设置OpenMP线程数为8\n",
    "import os\n",
    "import time\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import torch\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "\n",
    "# 从llama_index库导入HuggingFaceEmbedding类，用于将文本转换为向量表示\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# 从llama_index库导入ChromaVectorStore类，用于高效存储和检索向量数据\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# 从llama_index库导入PyMuPDFReader类，用于读取和解析PDF文件内容\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "# 从llama_index库导入NodeWithScore和TextNode类\n",
    "# NodeWithScore: 表示带有相关性分数的节点，用于排序检索结果\n",
    "# TextNode: 表示文本块，是索引和检索的基本单位。节点存储文本内容及其元数据，便于构建知识图谱和语义搜索\n",
    "from llama_index.core.schema import NodeWithScore, TextNode\n",
    "# 从llama_index库导入RetrieverQueryEngine类，用于协调检索器和响应生成，执行端到端的问答过程\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# 从llama_index库导入QueryBundle类，用于封装查询相关的信息，如查询文本、过滤器等\n",
    "from llama_index.core import QueryBundle\n",
    "# 从llama_index库导入BaseRetriever类，这是所有检索器的基类，定义了检索接口\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "# 从llama_index库导入SentenceSplitter类，用于将长文本分割成句子或语义完整的文本块，便于索引和检索\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# 从llama_index库导入VectorStoreQuery类，用于构造向量存储的查询，支持语义相似度搜索\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "# 向量数据库\n",
    "import chromadb\n",
    "from ipex_llm.llamaindex.llms import IpexLLM\n",
    "\n",
    "class Config:\n",
    "    \"\"\"配置类,存储所有需要的参数\"\"\"\n",
    "    model_path = \"qwen2chat_int4\"\n",
    "    tokenizer_path = \"qwen2chat_int4\"\n",
    "    # question = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "    data_path = \"./data/llamatiny.pdf\"\n",
    "    question = \"Can you summarize the idea mentioned in this paper LearningDenseRepresentation.pdf ？\"\n",
    "    # data_path = \"./data/LearningDenseRepresentation.pdf\"\n",
    "    persist_dir = \"./chroma_db\"\n",
    "    embedding_model_path = \"qwen2chat_src/AI-ModelScope/bge-small-zh-v1___5\"\n",
    "    max_new_tokens = 64\n",
    "\n",
    "def load_vector_database(persist_dir: str) -> ChromaVectorStore:\n",
    "    \"\"\"\n",
    "    加载或创建向量数据库\n",
    "    \n",
    "    Args:\n",
    "        persist_dir (str): 持久化目录路径\n",
    "    \n",
    "    Returns:\n",
    "        ChromaVectorStore: 向量存储对象\n",
    "    \"\"\"\n",
    "    # 检查持久化目录是否存在\n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"正在加载现有的向量数据库: {persist_dir}\")\n",
    "        chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "        chroma_collection = chroma_client.get_collection(\"llama2_paper\")\n",
    "    else:\n",
    "        print(f\"创建新的向量数据库: {persist_dir}\")\n",
    "        chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "        chroma_collection = chroma_client.create_collection(\"llama2_paper\")\n",
    "    print(f\"Vector store loaded with {chroma_collection.count()} documents\")\n",
    "    return ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "def load_data(data_path: str) -> List[TextNode]:\n",
    "    \"\"\"\n",
    "    加载并处理PDF数据\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): PDF文件路径\n",
    "    \n",
    "    Returns:\n",
    "        List[TextNode]: 处理后的文本节点列表\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFReader()\n",
    "    documents = loader.load(file_path=data_path)\n",
    "\n",
    "    text_parser = SentenceSplitter(chunk_size=384)\n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(text=text_chunk)\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"向量数据库检索器\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: ChromaVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        检索相关文档\n",
    "        \n",
    "        Args:\n",
    "            query_bundle (QueryBundle): 查询包\n",
    "        \n",
    "        Returns:\n",
    "            List[NodeWithScore]: 检索到的文档节点及其相关性得分\n",
    "        \"\"\"\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = self._vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "        print(f\"Retrieved {len(nodes_with_scores)} nodes with scores\")\n",
    "        return nodes_with_scores\n",
    "\n",
    "def completion_to_prompt(completion: str) -> str:\n",
    "    \"\"\"\n",
    "    将完成转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        completion (str): 完成的文本\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages: List[dict]) -> str:\n",
    "    \"\"\"\n",
    "    将消息列表转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        messages (List[dict]): 消息列表\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def setup_llm(config: Config) -> IpexLLM:\n",
    "    \"\"\"\n",
    "    设置语言模型\n",
    "    \n",
    "    Args:\n",
    "        config (Config): 配置对象\n",
    "    \n",
    "    Returns:\n",
    "        IpexLLM: 配置好的语言模型\n",
    "    \"\"\"\n",
    "    return IpexLLM.from_model_id_low_bit(\n",
    "        model_name=config.model_path,\n",
    "        tokenizer_name=config.tokenizer_path,\n",
    "        context_window=384,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "        model_kwargs={},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        device_map=\"cpu\",\n",
    "    )\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # 设置嵌入模型\n",
    "    embed_model = HuggingFaceEmbedding(model_name=config.embedding_model_path)\n",
    "    \n",
    "    # 设置语言模型\n",
    "    llm = setup_llm(config)\n",
    "    \n",
    "    # 加载向量数据库\n",
    "    vector_store = load_vector_database(persist_dir=config.persist_dir)\n",
    "    \n",
    "    # 加载和处理数据\n",
    "    nodes = load_data(data_path=config.data_path)\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "    \n",
    "    # 将 node 添加到向量存储\n",
    "    vector_store.add(nodes)\n",
    "    \n",
    "    # 设置查询\n",
    "    query_str = config.question\n",
    "    query_embedding = embed_model.get_query_embedding(query_str)\n",
    "    \n",
    "    # 执行向量存储检索\n",
    "    print(\"开始执行向量存储检索\")\n",
    "    query_mode = \"default\"\n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    "    )\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # 处理查询结果\n",
    "    print(\"开始处理检索结果\")\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "    \n",
    "    # 设置检索器\n",
    "    retriever = VectorDBRetriever(\n",
    "        vector_store, embed_model, query_mode=\"default\", similarity_top_k=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Query engine created with retriever: {type(retriever).__name__}\")\n",
    "    print(f\"Query string length: {len(query_str)}\")\n",
    "    print(f\"Query string: {query_str}\")\n",
    "    \n",
    "    # 创建查询引擎\n",
    "    print(\"准备与llm对话\")\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
    "\n",
    "    # 执行查询\n",
    "    print(\"开始RAG最后生成\")\n",
    "    start_time = time.time()\n",
    "    response = query_engine.query(query_str)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"------------RESPONSE GENERATION---------------------\")\n",
    "    print(str(response))\n",
    "    print(f\"inference time: {time.time()-start_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bd4ff-c22e-4262-bf09-717fb9cd31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/conda/envs/ipex/bin/python3 run_rag.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipex",
   "language": "python",
   "name": "ipex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
